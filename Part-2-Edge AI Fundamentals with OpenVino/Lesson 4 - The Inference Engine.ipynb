{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dive deeper into the Inference Engine, and perform inference in the OpenVINO Toolkit. By the end, you'll know the full workflow for OpenVINO fundamentals and be ready to integrate into an app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/BUpkwGhboLg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Inference Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/dZA4QGbDrs4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check on [Inference Engine Developer Guide](https://docs.openvinotoolkit.org/2019_R3/_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supported Devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/m2d1urdJegA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Use the Model Downloader and Model Optimizer for the Intel® Distribution of OpenVINO™ Toolkit on Raspberry Pi*](https://software.intel.com/content/www/us/en/develop/articles/model-downloader-optimizer-for-openvino-on-raspberry-pi.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check on [Supported Devices](https://docs.openvinotoolkit.org/2019_R3/_docs_IE_DG_supported_plugins_Supported_Devices.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Inference Engine with an IR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/b90ny0AmQF8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can review:\n",
    "- [IE Python API](https://docs.openvinotoolkit.org/2019_R3/ie_python_api.html)\n",
    "- [IE Network](https://docs.openvinotoolkit.org/2019_R3/classie__api_1_1IENetwork.html)\n",
    "- [IE Core](https://docs.openvinotoolkit.org/2019_R3/classie__api_1_1IECore.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Feed an IR to the Inference Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use IR in the Inference Engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Feed an IR to the Inference Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/jEmebNVBlc4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the necessary libraries\n",
    "import os\n",
    "from openvino.inference_engine import IENetwork, IECore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_IE(model_xml):\n",
    "    ### Load the Inference Engine API\n",
    "    plugin = IECore()\n",
    "\n",
    "    ### Load IR files into their related class\n",
    "    model_bin = os.path.splitext(model_xml)[0] + \".bin\"\n",
    "    net = IENetwork(model=model_xml, weights=model_bin)\n",
    "\n",
    "    ### Add a CPU extension, if applicable.\n",
    "    plugin.add_extension(CPU_EXTENSION, \"CPU\")\n",
    "\n",
    "    ### Get the supported layers of the network\n",
    "    supported_layers = plugin.query_network(network=net, device_name=\"CPU\")\n",
    "\n",
    "    ### Check for any unsupported layers, and let the user\n",
    "    ### know if anything is missing. Exit the program, if so.\n",
    "    unsupported_layers = [l for l in net.layers.keys() if l not in supported_layers]\n",
    "    if len(unsupported_layers) != 0:\n",
    "        print(\"Unsupported layers found: {}\".format(unsupported_layers))\n",
    "        print(\"Check whether extensions are available to add to IECore.\")\n",
    "        exit(1)\n",
    "\n",
    "    ### Load the network into the Inference Engine\n",
    "    plugin.load_network(net, \"CPU\")\n",
    "\n",
    "    print(\"IR successfully loaded into Inference Engine.\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Your Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python feed_network.py -m /home/workspace/models/human-pose-estimation-0001.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending Inference Requests to the IE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/wLN8HYZ05rg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we load the IENetwork into the IECore, we get back an ExecutableNetwork, which is what we will send inference requests to. There are two types of inference requests we can make: Synchronous and Asynchronous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an ExecutableNetwork, synchronous requests just use the infer function, while asynchronous requests begin with start_async, and then we can wait until the request is complete. These requests are InferRequest objects, which will hold both the input and output of the request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can review:\n",
    "- [Executable Network documentation](https://docs.openvinotoolkit.org/2019_R3/classie__api_1_1ExecutableNetwork.html)\n",
    "- [Infer Request documentation](https://docs.openvinotoolkit.org/2019_R3/classie__api_1_1InferRequest.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/JGuUIDpn1PY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synchronous requests wait and do nothing until the inference response is returned. They block the main thread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to asynchronous requests, we can continue processing while another process is working."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can review:\n",
    "- [About Synchronous vs. Asynchronous blog post](https://whatis.techtarget.com/definition/synchronous-asynchronous-API)\n",
    "- [Integrate the Inference Engine with Your Application](https://docs.openvinotoolkit.org/2019_R3/_docs_IE_DG_Integrate_with_customer_application_new_API.html)\n",
    "- [Asynchronous Inference Requests Demo](https://github.com/opencv/open_model_zoo/blob/master/demos/object_detection_demo_ssd_async/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Inference Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise is about requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Inference Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/QeBpEkkoZ74)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronous Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_inference(exec_net, input_blob, image):\n",
    "    '''\n",
    "    Performs synchronous inference\n",
    "    Return the result of inference\n",
    "    '''\n",
    "    result = exec_net.infer({input_blob: image})\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def async_inference(exec_net, input_blob, image):\n",
    "    '''\n",
    "    Performs asynchronous inference\n",
    "    Returns the `exec_net`\n",
    "    '''\n",
    "    exec_net.start_async(request_id=0, inputs={input_blob: image})\n",
    "    while True:\n",
    "        status = exec_net.requests[0].wait(-1)\n",
    "        if status == 0:\n",
    "            break\n",
    "        else:\n",
    "            time.sleep(1)\n",
    "    return exec_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't actually need time.sleep() here, because using the -1 with wait() is able to perform similar functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/wO_Io3wDwTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[InferenceEngine::Blob Class Reference](https://docs.openvinotoolkit.org/2019_R3/classInferenceEngine_1_1Blob.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating into Your App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/vQpLv1Y3pnU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can review:\n",
    "- [Intel®’s IoT Apps Across Industries](https://www.intel.com/content/www/us/en/internet-of-things/industry-solutions.html)\n",
    "- [Starting Your First IoT Project](https://hackernoon.com/the-ultimate-guide-to-starting-your-first-iot-project-8b0644fbbe6d)\n",
    "- [OpenVINO™ on a Raspberry Pi and Intel® Neural Compute Stick](https://www.pyimagesearch.com/2019/04/08/openvino-opencv-and-movidius-ncs-on-the-raspberry-pi/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Integrate into an App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the final exercise of this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Integrate into an App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/BIdLJkDD5vM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** There is a small change from the code on-screen for running on Linux machines versus Mac. On Mac, `cv2.VideoWriter` uses `cv2.VideoWriter_fourcc('M','J','P','G')` to write an .mp4 file, while Linux uses `0x00000021`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions in `inference.py`\n",
    "\n",
    "We use the async and wait functions here as it's split out slightly differently than we saw in the last exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, we need to know that output and input blobs were grabbed higher above when the network model is loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.input_blob = next(iter(self.network.inputs))\n",
    "self.output_blob = next(iter(self.network.outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use similar code as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def async_inference(self, image):\n",
    "    '''\n",
    "    Makes an asynchronous inference request, given an input image.\n",
    "    '''\n",
    "    self.exec_network.start_async(request_id=0, \n",
    "        inputs={self.input_blob: image})\n",
    "    return\n",
    "\n",
    "\n",
    "def wait(self):\n",
    "    '''\n",
    "    Checks the status of the inference request.\n",
    "    '''\n",
    "    status = self.exec_network.requests[0].wait(-1)\n",
    "    return status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can grab the network output using the appropriate request with the output_blob key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_output(self):\n",
    "    '''\n",
    "    Returns a list of the results for the output layer of the network.\n",
    "    '''\n",
    "    return self.exec_network.requests[0].outputs[self.output_blob]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions in `app.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next steps in app.py, before customization, are largely based on using the functions in inference.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize the Inference Engine\n",
    "plugin = Network()\n",
    "\n",
    "### Load the network model into the IE\n",
    "plugin.load_model(args.m, args.d, CPU_EXTENSION)\n",
    "net_input_shape = plugin.get_input_shape()\n",
    "\n",
    "...\n",
    "\n",
    "    ### Pre-process the frame\n",
    "    p_frame = cv2.resize(frame, (net_input_shape[3], net_input_shape[2]))\n",
    "    p_frame = p_frame.transpose((2,0,1))\n",
    "    p_frame = p_frame.reshape(1, *p_frame.shape)\n",
    "\n",
    "    ### Perform inference on the frame\n",
    "    plugin.async_inference(p_frame)\n",
    "\n",
    "    ### Get the output of inference\n",
    "    if plugin.wait() == 0:\n",
    "        result = plugin.extract_output()\n",
    "        ### Update the frame to include detected bounding boxes\n",
    "        frame = draw_boxes(frame, result, args, width, height)\n",
    "        # Write out the frame\n",
    "        out.write(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The draw_boxes function is used to extract the bounding boxes and draw them back onto the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(frame, result, args, width, height):\n",
    "    '''\n",
    "    Draw bounding boxes onto the frame.\n",
    "    '''\n",
    "    for box in result[0][0]: # Output shape is 1x1x100x7\n",
    "        conf = box[2]\n",
    "        if conf >= 0.5:\n",
    "            xmin = int(box[3] * width)\n",
    "            ymin = int(box[4] * height)\n",
    "            xmax = int(box[5] * width)\n",
    "            ymax = int(box[6] * height)\n",
    "            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 0, 255), 1)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing `app.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing the command line arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_desc = \"The color of the bounding boxes to draw; RED, GREEN or BLUE\"\n",
    "ct_desc = \"The confidence threshold to use with the bounding boxes\"\n",
    "\n",
    "# ...\n",
    "\n",
    "optional.add_argument(\"-c\", help=c_desc, default='BLUE')\n",
    "optional.add_argument(\"-ct\", help=ct_desc, default=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle the new arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_color(color_string):\n",
    "    '''\n",
    "    Get the BGR value of the desired bounding box color.\n",
    "    Defaults to Blue if an invalid color is given.\n",
    "    '''\n",
    "    colors = {\"BLUE\": (255,0,0), \"GREEN\": (0,255,0), \"RED\": (0,0,255)}\n",
    "    out_color = colors.get(color_string)\n",
    "    if out_color:\n",
    "        return out_color\n",
    "    else:\n",
    "        return colors['BLUE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to call this with the related argument, as well as make sure the confidence threshold argument is a float value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.c = convert_color(args.c)\n",
    "args.ct = float(args.ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding customization to `draw_boxes()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = draw_boxes(frame, result, args, width, height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use them where appropriate in the updated function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(frame, result, args, width, height):\n",
    "    '''\n",
    "    Draw bounding boxes onto the frame.\n",
    "    '''\n",
    "    for box in result[0][0]: # Output shape is 1x1x100x7\n",
    "        conf = box[2]\n",
    "        if conf >= args.ct:\n",
    "            xmin = int(box[3] * width)\n",
    "            ymin = int(box[4] * height)\n",
    "            xmax = int(box[5] * width)\n",
    "            ymax = int(box[6] * height)\n",
    "            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), args.c, 1)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We run the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python app.py -m frozen_inference_graph.xml -ct 0.6 -c BLUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behind the Scenes of Inference Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/ZWpNQjXSEEc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can review:\n",
    "- [What is the best programming language for Machine Learning? - Blog Post](https://towardsdatascience.com/what-is-the-best-programming-language-for-machine-learning-a745c156d6b7)\n",
    "- [Optimization Guide ](https://docs.openvinotoolkit.org/2019_R3/_docs_optimization_guide_dldt_optimization_guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/AVmFgZyk0T0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Glossary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performs optimized inference using Intermediate Representation models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [IECore](https://docs.openvinotoolkit.org/2019_R3/classie__api_1_1IECore.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [IENetwork](https://docs.openvinotoolkit.org/2019_R3/classie__api_1_1IENetwork.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ExecutableNetwork](https://docs.openvinotoolkit.org/2019_R3/classie__api_1_1ExecutableNetwork.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [InferRequest](https://docs.openvinotoolkit.org/2019_R3/classie__api_1_1InferRequest.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
