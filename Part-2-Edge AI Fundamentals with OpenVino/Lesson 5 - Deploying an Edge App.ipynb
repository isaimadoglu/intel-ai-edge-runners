{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the OpenVINO Toolkit fundamentals down, you're ready to move onto more topics to get your edge app up and running. Learn about handling input streams, MQTT and more as you finish the tutorial!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/IBP4tsdFRPg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenCV Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/c-pyJ3XwWko)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check on [OpenCV Tutorials](https://docs.opencv.org/master/d9/df8/tutorial_root.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Input Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/F-7ofR4pdNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Handling Input Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement a function that can handle camera, video or webcam data as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Handling Input Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/p6Hd3dnf-LY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** There are two small changes from the code on video for running on Linux machines versus Mac.\n",
    "\n",
    "- On Mac, `cv2.VideoWriter` uses `cv2.VideoWriter_fourcc('M','J','P','G')` to write an `.mp4` file, while Linux uses `0x00000021`.\n",
    "- On Mac, the output with the given code on using Canny Edge Detection will run fine. However, on Linux, you'll need to use `np.dstack` to make a 3-channel array to write back to the `out` file, or else the video won't be able to be opened correctly: `frame = np.dstack((frame, frame, frame))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `cv2.VideoCapture()` and open the capture stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture(input_stream)\n",
    "capture.open(args.input)\n",
    "\n",
    "while capture.isOpened():\n",
    "    flag, frame = cap.read()\n",
    "    if not flag:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bit outside of the instructions, but it's also important to check whether a key gets pressed within the while loop, to make it easier to exit.\n",
    "\n",
    "We can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check for a keypress\n",
    "key_pressed = cv2.waitKey(60)\n",
    "\n",
    "# and then to break the loop, if needed. Key 27 is the Escape button."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-size the frame to 100x100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.resize(frame, (100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = cv2.Canny(image,100,200)\n",
    "\n",
    "# Display the resulting frame if it's video, or save it if it is an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('display', edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite('output.jpg', edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the stream and any windows at the end of the application\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python app.py -i blue-car.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python app.py -i test_video.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Useful Information from Model Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/uNoIZI9bm6U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Process Model Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use video input and process model outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import cv2\n",
    "from inference import Network\n",
    "\n",
    "INPUT_STREAM = \"pets.mp4\"\n",
    "CPU_EXTENSION = \"/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_sse4.so\"\n",
    "\n",
    "def get_args():\n",
    "    '''\n",
    "    Gets the arguments from the command line.\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser(\"Run inference on an input video\")\n",
    "    # -- Create the descriptions for the commands\n",
    "    m_desc = \"The location of the model XML file\"\n",
    "    i_desc = \"The location of the input file\"\n",
    "    d_desc = \"The device name, if not 'CPU'\"\n",
    "\n",
    "    # -- Add required and optional groups\n",
    "    parser._action_groups.pop()\n",
    "    required = parser.add_argument_group('required arguments')\n",
    "    optional = parser.add_argument_group('optional arguments')\n",
    "\n",
    "    # -- Create the arguments\n",
    "    required.add_argument(\"-m\", help=m_desc, required=True)\n",
    "    optional.add_argument(\"-i\", help=i_desc, default=INPUT_STREAM)\n",
    "    optional.add_argument(\"-d\", help=d_desc, default='CPU')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n",
    "def assess_scene(result, counter, incident_flag):\n",
    "    '''\n",
    "    Based on the determined situation, potentially send\n",
    "    a message to the pets to break it up.\n",
    "    '''\n",
    "    if result[0][1] == 1 and not incident_flag:\n",
    "        timestamp = counter / 30\n",
    "        print(\"Log: Incident at {:.2f} seconds.\".format(timestamp))\n",
    "        print(\"Break it up!\")\n",
    "        incident_flag = True\n",
    "    elif result[0][1] != 1:\n",
    "        incident_flag = False\n",
    "    \n",
    "    return incident_flag\n",
    "\n",
    "def infer_on_video(args):\n",
    "    # Initialize the Inference Engine\n",
    "    plugin = Network()\n",
    "\n",
    "    # Load the network model into the IE\n",
    "    plugin.load_model(args.m, args.d, CPU_EXTENSION)\n",
    "    net_input_shape = plugin.get_input_shape()\n",
    "\n",
    "    # Get and open video capture\n",
    "    cap = cv2.VideoCapture(args.i)\n",
    "    cap.open(args.i)\n",
    "\n",
    "    # Process frames until the video ends, or process is exited\n",
    "    counter = 0\n",
    "    incident_flag = False\n",
    "    while cap.isOpened():\n",
    "        # Read the next frame\n",
    "        flag, frame = cap.read()\n",
    "        if not flag:\n",
    "            break\n",
    "        key_pressed = cv2.waitKey(60)\n",
    "        counter += 1\n",
    "\n",
    "        # Pre-process the frame\n",
    "        p_frame = cv2.resize(frame, (net_input_shape[3], net_input_shape[2]))\n",
    "        p_frame = p_frame.transpose((2,0,1))\n",
    "        p_frame = p_frame.reshape(1, *p_frame.shape)\n",
    "\n",
    "        # Perform inference on the frame\n",
    "        plugin.async_inference(p_frame)\n",
    "\n",
    "        # Get the output of inference\n",
    "        if plugin.wait() == 0:\n",
    "            result = plugin.extract_output()\n",
    "            ### TODO: Process the output\n",
    "            incident_flag = assess_scene(result, counter, incident_flag)\n",
    "            \n",
    "        # Break if escape key pressed\n",
    "        if key_pressed == 27:\n",
    "            break\n",
    "\n",
    "    # Release the capture and destroy any OpenCV windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = get_args()\n",
    "    infer_on_video(args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Process Model Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/s35d7IvQliE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the video loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "incident_flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_scene(result, counter, incident_flag):\n",
    "    '''\n",
    "    Based on the determined situation, potentially send\n",
    "    a message to the pets to break it up.\n",
    "    '''\n",
    "    if result[0][1] == 1 and not incident_flag:\n",
    "        timestamp = counter / 30\n",
    "        print(\"Log: Incident at {:.2f} seconds.\".format(timestamp))\n",
    "        print(\"Break it up!\")\n",
    "        incident_flag = True\n",
    "    elif result[0][1] != 1:\n",
    "        incident_flag = False\n",
    "\n",
    "    return incident_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we call the function the loop right after the result is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incident_flag = assess_scene(result, counter, incident_flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python app.py -m model.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to MQTT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/xe8137UtNCQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can review:\n",
    "- [Main site for MQTT](http://mqtt.org/)\n",
    "- [A helpful post about MQTT basics](https://internetofthingsagenda.techtarget.com/definition/MQTT-MQ-Telemetry-Transport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Communicating with MQTT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/3Jh-akmlmmU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can review:\n",
    "- [`paho-mqtt` Python library documentation](https://pypi.org/project/paho-mqtt/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Images to a Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/MyutnkUqIv4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can review:\n",
    "- [FFMPEG](https://www.ffmpeg.org/)\n",
    "- [Set up Your Own Server on Linux](https://opensource.com/article/19/1/basic-live-video-streaming-server)\n",
    "- [Use Flask and Python for Streaming](https://www.pyimagesearch.com/2019/09/02/opencv-stream-video-to-web-browser-html-page/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Statistics and Images from a Node Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/qHQ9HjmRGWw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can review:\n",
    "- [Node.js](https://nodejs.org/en/about/)\n",
    "- [Front End Developer Nanodegree program](https://www.udacity.com/course/front-end-web-developer-nanodegree--nd0011)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Server Communications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use MQTT and FFMPEG in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import cv2\n",
    "import sys\n",
    "import numpy as np\n",
    "import socket\n",
    "import json\n",
    "import paho.mqtt.client as mqtt\n",
    "from random import randint\n",
    "from inference import Network\n",
    "### TODO: Import any libraries for MQTT and FFmpeg\n",
    "\n",
    "INPUT_STREAM = \"test_video.mp4\"\n",
    "CPU_EXTENSION = \"/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_sse4.so\"\n",
    "ADAS_MODEL = \"/home/workspace/models/semantic-segmentation-adas-0001.xml\"\n",
    "\n",
    "\n",
    "CLASSES = ['road', 'sidewalk', 'building', 'wall', 'fence', 'pole', \n",
    "'traffic_light', 'traffic_sign', 'vegetation', 'terrain', 'sky', 'person',\n",
    "'rider', 'car', 'truck', 'bus', 'train', 'motorcycle', 'bicycle', 'ego-vehicle']\n",
    "\n",
    "# MQTT server environment variables\n",
    "HOSTNAME = socket.gethostname()\n",
    "IPADDRESS = socket.gethostbyname(HOSTNAME)\n",
    "MQTT_HOST = IPADDRESS\n",
    "MQTT_PORT = 3004 ### TODO: Set the Port for MQTT\n",
    "MQTT_KEEPALIVE_INTERVAL = 60\n",
    "\n",
    "def get_args():\n",
    "    '''\n",
    "    Gets the arguments from the command line.\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser(\"Run inference on an input video\")\n",
    "    # -- Create the descriptions for the commands\n",
    "    i_desc = \"The location of the input file\"\n",
    "    d_desc = \"The device name, if not 'CPU'\"\n",
    "\n",
    "    # -- Create the arguments\n",
    "    parser.add_argument(\"-i\", help=i_desc, default=INPUT_STREAM)\n",
    "    parser.add_argument(\"-d\", help=d_desc, default='CPU')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def draw_masks(result, width, height):\n",
    "    '''\n",
    "    Draw semantic mask classes onto the frame.\n",
    "    '''\n",
    "    # Create a mask with color by class\n",
    "    classes = cv2.resize(result[0].transpose((1,2,0)), (width,height), \n",
    "        interpolation=cv2.INTER_NEAREST)\n",
    "    unique_classes = np.unique(classes)\n",
    "    out_mask = classes * (255/20)\n",
    "    \n",
    "    # Stack the mask so FFmpeg understands it\n",
    "    out_mask = np.dstack((out_mask, out_mask, out_mask))\n",
    "    out_mask = np.uint8(out_mask)\n",
    "\n",
    "    return out_mask, unique_classes\n",
    "\n",
    "\n",
    "def get_class_names(class_nums):\n",
    "    class_names= []\n",
    "    for i in class_nums:\n",
    "        class_names.append(CLASSES[int(i)])\n",
    "    return class_names\n",
    "\n",
    "\n",
    "def infer_on_video(args, model):\n",
    "    ### TODO: Connect to the MQTT server\n",
    "    client = mqtt.Client()\n",
    "    client.connect(MQTT_HOST, MQTT_PORT,\n",
    "                  MQTT_KEEPALIVE_INTERVAL)\n",
    "\n",
    "    # Initialize the Inference Engine\n",
    "    plugin = Network()\n",
    "\n",
    "    # Load the network model into the IE\n",
    "    plugin.load_model(model, args.d, CPU_EXTENSION)\n",
    "    net_input_shape = plugin.get_input_shape()\n",
    "\n",
    "    # Get and open video capture\n",
    "    cap = cv2.VideoCapture(args.i)\n",
    "    cap.open(args.i)\n",
    "\n",
    "    # Grab the shape of the input \n",
    "    width = int(cap.get(3))\n",
    "    height = int(cap.get(4))\n",
    "\n",
    "    # Process frames until the video ends, or process is exited\n",
    "    while cap.isOpened():\n",
    "        # Read the next frame\n",
    "        flag, frame = cap.read()\n",
    "        if not flag:\n",
    "            break\n",
    "        key_pressed = cv2.waitKey(60)\n",
    "\n",
    "        # Pre-process the frame\n",
    "        p_frame = cv2.resize(frame, (net_input_shape[3], net_input_shape[2]))\n",
    "        p_frame = p_frame.transpose((2,0,1))\n",
    "        p_frame = p_frame.reshape(1, *p_frame.shape)\n",
    "\n",
    "        # Perform inference on the frame\n",
    "        plugin.async_inference(p_frame)\n",
    "\n",
    "        # Get the output of inference\n",
    "        if plugin.wait() == 0:\n",
    "            result = plugin.extract_output()\n",
    "            # Draw the output mask onto the input\n",
    "            out_frame, classes = draw_masks(result, width, height)\n",
    "            class_names = get_class_names(classes)\n",
    "            speed = randint(50,70)\n",
    "            \n",
    "            ### TODO: Send the class names and speed to the MQTT server\n",
    "            client.publish(\"class\", json.dumps({\"class_names\": class_names}))\n",
    "            client.publish(\"speedometer\", json.dumps({\"speed\": speed}))\n",
    "            ### Hint: The UI web server will check for a \"class\" and\n",
    "            ### \"speedometer\" topic. Additionally, it expects \"class_names\"\n",
    "            ### and \"speed\" as the json keys of the data, respectively.\n",
    "            \n",
    "\n",
    "        ### TODO: Send frame to the ffmpeg server\n",
    "        sys.stdout.buffer.write(out_frame)\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "\n",
    "        # Break if escape key pressed\n",
    "        if key_pressed == 27:\n",
    "            break\n",
    "\n",
    "    # Release the capture and destroy any OpenCV windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    ### TODO: Disconnect from MQTT\n",
    "    client.disconnect()\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = get_args()\n",
    "    model = ADAS_MODEL\n",
    "    infer_on_video(args, model)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Server Communications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/c2cNJgrvHmg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MQTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paho.mqtt.client as mqtt\n",
    "import socket  # for connection with the MQTT server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This will set the IP address and port, \n",
    "as well as the keep alive interval. \n",
    "The keep alive interval is used so that the server \n",
    "and client will communicate every 60 seconds \n",
    "to confirm their connection is still open, \n",
    "if no other communication (such as the inference statistics) \n",
    "is received.\n",
    "\"\"\"\n",
    "HOSTNAME = socket.gethostname()\n",
    "IPADDRESS = socket.gethostbyname(HOSTNAME)\n",
    "MQTT_HOST = IPADDRESS\n",
    "MQTT_PORT = 3001\n",
    "MQTT_KEEPALIVE_INTERVAL = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The port here is 3001, instead of the normal MQTT port of 1883, as Udacity's classroom workspace environment only allows ports from 3000-3009 to be used. The real importance is here to make sure this matches to what is set for the MQTT broker server to be listening on, which in this case has also been set to 3001 (we can see this in `config.js` within the MQTT server's files in the workspace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to the client\n",
    "client = mqtt.Client()\n",
    "client.connect(MQTT_HOST, MQTT_PORT, MQTT_KEEPALIVE_INTERVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publishing the statistics to the connected MQTT client\n",
    "topic = \"some_string\"\n",
    "client.publish(topic, json.dumps({\"stat_name\": statistic}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topic here should match to the relevant topic that is being subscribed to from the other end, while the JSON being published should include the relevant name of the statistic for the node server to parse (with the name like the key of a dictionary), with the statistic passed in with it (like the items of a dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.publish(\"class\", json.dumps({\"class_names\": class_names}))\n",
    "client.publish(\"speedometer\", json.dumps({\"speed\": speed}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disconnecting at the end of processing the input stream\n",
    "client.disconnect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FFmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FFmpeg does not actually have any real specific imports, although we want the standard `sys` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is used as the `ffserver` can be configured to read from `sys.stdout`. Once the output frame has been processed (drawing bounding boxes, semantic masks, etc.), we can write the frame to the stdout buffer and flush it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stdout.buffer.write(frame)  \n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! As long as the MQTT and FFmpeg servers are running and configured appropriately, the information should be able to be received by the final node server, and viewed in the browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the app itself, with the UI server, MQTT server, and FFmpeg server also running, we use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python app.py | ffmpeg -v warning -f rawvideo -pixel_format bgr24 -video_size 1280x720 -framerate 24 -i - http://0.0.0.0:3004/fac.ffm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will feed the output of the app to FFmpeg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Performance Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/bGPyVQnLnIw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can review:\n",
    "- [Introduction to the Performance Topics](https://docs.openvinotoolkit.org/2019_R3/_docs_IE_DG_Intro_to_Performance.html)\n",
    "- [Netflix uses 15% of worldwide bandwidth with its video streaming](https://www.sandvine.com/hubfs/downloads/phenomena/phenomena-presentation-final.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/rQ0980eYeeA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check on [Deep Learning for Distracted Driving Detection](https://www.nauto.com/blog/nauto-engineering-deep-learning-for-distracted-driver-monitoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concerning End User Needs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/aHtne0LexrA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/6egpCIvzUl8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Glossary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A computer vision (CV) library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MQTT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A publisher-subscriber protocol. \"paho-mqtt\" library is a common way of working with MQTT in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish-Subscribe Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publisher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subscriber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Python framework useful for web development and another potential option for video streaming to a web browser. [Flask](https://www.fullstackpython.com/flask.html) is used also to build APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A web server built with Node.js that can handle HTTP requests and/or serve up a webpage for viewing in a browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/27C3zdKmt_M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intel® DevMesh\n",
    "\n",
    "Check out the [Intel® DevMesh](https://devmesh.intel.com/) website for some more awesome projects others have built, join in on existing projects, or even post some of your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can [download the OpenVINO toolkit](https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/choose-download.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partner with Intel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep growing your Edge AI skills with the resources below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get Started - [Download OpenVINO Today!](https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/choose-download.html?Utm_Medium=Udacity_Platform&utm_source=udacity_fundamentals&utm_content=getstarteddnl&utm_campaign=udacity_2020&cid=certifications&campID=asmo-nar-openvino-da)\n",
    "- Connect with Experts - [OpenVINO Toolkit Forum](https://software.intel.com/en-us/forums/intel-distribution-of-openvino-toolkit?Utm_Medium=Udacity_Platform&utm_source=udacity_fundamentals&utm_content=connecttlkit&utm_campaign=udacity_2020&cid=certifications&campID=asmo-nar-openvino-da)\n",
    "- Develop an app using the OpenVINO toolkit - [Access self-paced development guide](https://www.intel.com/content/www/us/en/develop/topics/iot/training/go-to-market-with-openvino.html#linklist_543792205?Utm_Medium=Udacity_Platform&utm_source=udacity_fundamentals&utm_content=devappguide&utm_campaign=udacity_2020&cid=certifications&campID=asmo-nar-openvino-da)\n",
    "- Get Access to cutting edge AI Hardware with [Intel DevCloud for the Edge](https://software.intel.com/content/www/us/en/develop/tools/devcloud/edge.html?%20utm_medium=Udacity_Platform&utm_source=udacity_fundamentals&utm_content=devcloud&utm_campaign=udacity_2020&cid=certifications&campID=asmo-nar-openvino-da). Develop, test and run your workloads for free on the latest Intel hardware and software."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
