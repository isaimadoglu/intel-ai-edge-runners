{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize Pre-Trained Models from the Intel Distribution of OpenVINO Toolkit to build powerful edge applications, without the need to train our own model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/vFNZu1VpdwE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lesson 2 covers subjects below:\n",
    "* Intel OpenVINO Toolkit basics\n",
    "* Different Computer Vision model types\n",
    "* Available Pre-Trained Models in the Software\n",
    "* Choosing the right Pre-Trained Model for our Application\n",
    "* Loading and Deploying a Basic Application with a Pre-Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The OpenVINO™ Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/-pM9pLCnzJk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General information about OpenVINO Toolkit is expressed here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OpenVINO Toolkit's name comes from \"Open Visual Inferencing and Neural Network Optimization\". This open-source software is largely focused around optimizing neural network inference.\n",
    "\n",
    "The software is developed by Intel and helps support fast inference across Intel CPUs, GPUs, FPGAs and Neural Compute Stick with a common API.\n",
    "\n",
    "OpenVINO supports models that trained some frameworks like TensorFlow or Caffe to use in Model Optimizer. Model Optimizer converts models into Intermediate Representation models. Intermediate Representation models can then be used with the Inference Engine, which helps speed inference on the related hardware. The toolkit also has a wide variety of Pre-Trained Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Optimizer optimizes model speed and size so as to make models deployable on edge applications. This optimization does not increase inference accuracy - this needs to be done in training beforehand. Lower resource applications need smaller, quicker models and hardware optimizations. OpenVINO Toolkit provides them. For example, an IoT device does not have the benefit of multiple GPUs and unlimited memory space to run its apps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Intel Distribution of OpenVINO Toolkit is an open source library useful for edge deployment due to its performance maximizations and pre-trained models.\n",
    "\n",
    "[Main site of OpenVINO Toolkit](https://software.intel.com/en-us/openvino-toolkit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Trained Models in OpenVINO™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/1-Vije0cMBQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " There are lots of Pre-Trained Models directly in the software. Pre-Trained Models are previously trained models with high accuracy. If we find a Pre-Trained Model, then we don't need to collect data and train the model from the beginning. We learn how to preprocess inputs and handle the outputs, then we plug the model into our application.\n",
    "\n",
    " Pre-Trained Models refer specifically to the Model Zoo, containing models that already converted using the Model Optimizer. As such, we can use these models directly with the Inference Engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Process](https://software.intel.com/sites/default/files/managed/ed/e9/inference-engine-700w-300h.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pre-Trained Models and documentations of them](https://software.intel.com/en-us/openvino-toolkit/documentation/pretrained-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Computer Vision Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/E8yBgSKfCoo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part selected three types of computer vision models are covered: Classification, Detection and Segmentation.\n",
    "\n",
    "**Classification** predicts \"class\" of an image, or an object in an image. Prediction is determined according to \"probability\" of classes, the highest probability is determined class, but you can also see the top 5 predictions as well.\n",
    "\n",
    "**Detection** is used when we want to detect the objects that appear at different places in an image. Detection applications generally draw bounding boxes around the detected objects. We can say that the applications which find objects and their locations in an image is a Detection application. It also usually has some form of classification that determines the class of an object in a given bounding box. The bounding boxes have a confidence threshold so you can throw out low-confidence detections.\n",
    "\n",
    "**Segmentation** classifies sections of an image by classifying each and every pixel. These networks are often post-processed in some way to avoid phantom classes here and there. \n",
    "\n",
    "There are also *subsets of segmentation*: Semantic Segmentation and Instance Segmentation. Semantic Segmentation considers all instances of a class as one, while Instance segmentation actually consider separate instances of a class as separate objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a useful [Medium post](https://medium.com/analytics-vidhya/image-classification-vs-object-detection-vs-image-segmentation-f36db85fe81) if you want to go a little further on types of computer vision models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Studies in Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/7mUaovlA4aQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SSD, ResNet and MobileNet neural network structures are expressed in this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SSD is an object detection network that combined classification with object detection through the use of default bounding boxes at different network levels.\n",
    "\n",
    "ResNet utilized residual layers to \"skip\" over sections of layers, helping to avoid the vanishing gradient problem with very deep neural networks.\n",
    "\n",
    "MobileNet utilized layers like 1x1 convolutions to help cut down on computational complexity and network size, leading to fast inference without substantial decrease in accuracy.\n",
    "\n",
    "One additional note here on the ResNet architecture - the paper itself actually theorizes that very deep neural networks have convergence issues due to exponentially lower convergence rates, as opposed to just the vanishing gradient problem. The vanishing gradient problem is also thought to be helped by the use of normalization of inputs to each different layer, which is not specific to ResNet. The ResNet architecture itself, at multiple different numbers of layers, was shown to converge faster during training than a \"plain\" network without the residual layers.    \n",
    "    \n",
    "[Single Shot Multibox Detector (SSD)](https://arxiv.org/abs/1512.02325) performs classification operations on different convolutional layer feature maps using default bounding boxes.\n",
    "\n",
    "The \"residual learning\" achieved in the [ResNet](https://arxiv.org/pdf/1512.03385.pdf) model architecture is achieved by using \"skip\" layers that pass information forward by a couple of layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting used to reading research papers is a key skill to build when working with AI and Computer Vision. Below, we can find the original research papers on some of the networks we discussed in this section.\n",
    "* [SSD](https://arxiv.org/abs/1512.02325)\n",
    "* [YOLO](https://arxiv.org/abs/1506.02640)\n",
    "* [Faster RCNN](https://arxiv.org/abs/1506.01497)\n",
    "* [MobileNet](https://arxiv.org/abs/1704.04861)\n",
    "* [ResNet](https://arxiv.org/abs/1512.03385)\n",
    "* [Inception](https://arxiv.org/pdf/1409.4842.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Pre-Trained Models in OpenVINO™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/SoTH1jr3-HA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Trained Models are the models that trained already. We can use them on Text Detection, Pose Detection, Roadside Segmentation, Pedestrian Detection etc. problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two kind of Pre-Trained Models on OpenVINO toolkit: Public Model Set and Free Model Set.\n",
    "- **Public Model Set** includes models that haven't been used on Model Optimizer, so they can be used to fine tune or use on Model Optimizer.\n",
    "- **Free Model Set**  inculdes models that have been used on Model Optimizer and converted into Intermediate Representation so that we can use it on the Inference Engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fetch the models using OpenVINO Model Downloader tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pretrained Models page](https://software.intel.com/en-us/openvino-toolkit/documentation/pretrained-models) of the Intel® Distribution of OpenVINO™ toolkit can be reviewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Loading Pre-Trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to download and load some of the pre-trained models available in the OpenVINO toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we look at [Pre-Trained Models page](https://software.intel.com/en-us/openvino-toolkit/documentation/pretrained-models), then we can see the [full list of available models](https://docs.openvinotoolkit.org/latest/_models_intel_index.html).\n",
    "\n",
    "**NOTE:** It is not necessary to download all of the models. We can download what we need.\n",
    "\n",
    "**HINT:** We can use the `-h` command with the Model Downloader tool whenever we need to check out the possible arguments to use when we want to download a specific model and its precisions.\n",
    "\n",
    "**HINT:** We can use `-o` argument if we want to download the model into a different directory instead of the default location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go to the default **Model Downloader location** with the command below;\n",
    "\n",
    "`cd /opt/intel/openvino/deployment_tools/open_model_zoo/tools/downloader/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - We find the right models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are responsible for the tasks below:\n",
    "- Human pose estimation\n",
    "- Text detection\n",
    "- Determining car type & color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find them on the [Pre-Trained Models page](https://software.intel.com/en-us/openvino-toolkit/documentation/pretrained-models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - We download the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we determine the models according to the tasks above, we download the models according to the precision levels below:\n",
    "- Human pose estimation: All precision levels\n",
    "- Text detection: FP16 only\n",
    "- Determining car type & color: INT8 only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 - We verify the downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify our downloads by checking the download location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `ls` command to check it.\n",
    "\n",
    "`ls /opt/intel/openvino/deployment_tools/open_model_zoo/tools/downloader/intel/`\n",
    "\n",
    "**NOTE:** The path we see here is the default path, if we use `-o` argument, the path can be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Loading Pre-Trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/QMfTUdWFsGw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** In OpenVINO 2019R3 version, `INT8` precision was used, but this precision re-named to `FP32-INT8` in 2020R1 version.\n",
    "\n",
    "Working locally with 2020R1, the download doesn't fail if we specify `INT8`, but the related download directory will be empty. Therefore we need to specify `FP32-INT8` as the `--precisions` argument if we use 2020R1 version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Models\n",
    "We can choose the models below for our tasks:\n",
    "- Human pose estimation: [human-pose-estimation-0001](https://docs.openvinotoolkit.org/latest/_models_intel_human_pose_estimation_0001_description_human_pose_estimation_0001.html)\n",
    "- Text detection: [text-detection-0004](http://docs.openvinotoolkit.org/latest/_models_intel_text_detection_0004_description_text_detection_0004.html)\n",
    "- Determining car type & color: [vehicle-attributes-recognition-barrier-0039](https://docs.openvinotoolkit.org/latest/_models_intel_vehicle_attributes_recognition_barrier_0039_description_vehicle_attributes_recognition_barrier_0039.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Models\n",
    "We can go to the default **Model Downloader location** with the command below;\n",
    "\n",
    "`cd /opt/intel/openvino/deployment_tools/open_model_zoo/tools/downloader/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see `downloader.py` file and we can use `-h` argument with it to see available arguments.\n",
    "\n",
    "`sudo ./downloader.py -h`  \n",
    "\n",
    "**Note that** `downloader.py` uses the interpreter determined in the file (`python3`) with shebang. I tried to interpret that file using Anaconda environment and got errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we use `--name` argument for model name and `--precisions` argument when we need only specific precisions. \n",
    "\n",
    "**Note:** Running `downloader.py` without arguments will download ***all*** available pre-trained models. This means that we download gigabytes of files if we do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If we are on local, we can use the commands below directly, but if we want to download them on different location, we can add `-o ~/openvino-models` at the end of the commands to download them on our home directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading Human Pose Model\n",
    "`sudo ./downloader.py --name human-pose-estimation-0001`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading Text Detection Model\n",
    "`sudo ./downloader.py --name text-detection-0004 --precisions FP16`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading Car Metadata Model\n",
    "`sudo ./downloader.py --name vehicle-attributes-recognition-barrier-0039 --precisions FP32-INT8`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Downloads\n",
    "The downloader tells us the directories that hold our models, but to verify them, we can use `ls` command.\n",
    "\n",
    "We can check the directories in the download location. Each directory of our three models must include subdirectories for each precision, with respective `.bin` and `.xml` for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizations on the Pre-Trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/nKvZYnOnWm4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precisions that we used in the previous exercise are related to floating point values. Less precision means less memory used by the model and less compute resources. However, there are some trade-offs with accuracy when using lower precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use fusion, which we fuse multiple layers into a single operation. We can achieve some optimizations through the Model Optimizer in OpenVINO. We will see these optimization techniques in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Right Model for Your App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/CWC195DzgAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose the right model according to criterions we specify for our application.\n",
    "\n",
    "These criterions can be:\n",
    "- Performance\n",
    "- Speed\n",
    "\n",
    "For example we can use the `Pedestrian Detection` pre-trained model to use on our waiter robot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/E9huKos96Uk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The needed pre-processing can be done using documentation of a model. We can also use OpenVINO Toolkit documentation if we want to use a pre-trained model from OpenVINO Model Zoo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to be careful while using a library to load an image or frame. We can use OpenCV in this tutorial. OpenCV reads images and frames in the BGR format, so images must be converted into RGB format if we want to use them on some networks that trained with RGB images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make another pre-processing operations besides channel order for our Computer Vision based models, such as:\n",
    "- Image size (Input images must be the same size with determined size)\n",
    "- Order of the image data (Color channels may come before or after the dimensions of the image)\n",
    "- Pixel normalization (Models may require Pixel values between 0 and 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In OpenCV, we can use `cv2.imread` to read images in BGR format, and `cv2.resize` to resize them. The images are hold as a Numpy array, so we can also use array functions like `.transpose` and `.reshape` on them. So we can switch the dimension orders of arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Pre-processing Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preprocess the inputs according to expectations of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a few pre-trained models from the previous exercise. It is time to preprocess the inputs to fit inputs with models' expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our three model documentations are here:\n",
    "- Human pose estimation: [human-pose-estimation-0001](https://docs.openvinotoolkit.org/latest/_models_intel_human_pose_estimation_0001_description_human_pose_estimation_0001.html)\n",
    "- Text detection: [text-detection-0004](http://docs.openvinotoolkit.org/latest/_models_intel_text_detection_0004_description_text_detection_0004.html)\n",
    "- Determining car type & color: [vehicle-attributes-recognition-barrier-0039](https://docs.openvinotoolkit.org/latest/_models_intel_vehicle_attributes_recognition_barrier_0039_description_vehicle_attributes_recognition_barrier_0039.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the **Inputs** sections of the documentations. \n",
    "\n",
    "Input sections include:\n",
    "- input shape, \n",
    "- order of the shape (i.e. color channel order)\n",
    "- order of the color channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task is to code three functions within `preprocess_inputs.py`, one for each of the three models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have potential sample images for the models.\n",
    "- [For Human pose estimation](https://cdn.pixabay.com/photo/2014/02/15/12/22/figure-skater-266512_960_720.jpg)\n",
    "- [For Text detection](https://cdn.pixabay.com/photo/2016/11/21/15/13/blue-1845901_960_720.jpg)\n",
    "- [For Determining car type & color](https://cdn.pixabay.com/photo/2015/05/15/14/46/bmw-768688_960_720.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We must assume in this exercise that images are loaded using OpenCV, so they come us as BGR format with Height, Width, Channel order. We make pre-processing according to this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def pose_estimation(input_image):\n",
    "    '''\n",
    "    Given some input image, preprocess the image so that\n",
    "    it can be used with the related pose estimation model\n",
    "    you downloaded previously. You can use cv2.resize()\n",
    "    to resize the image.\n",
    "    '''\n",
    "    preprocessed_image = np.copy(input_image)\n",
    "\n",
    "    # TODO: Preprocess the image for the pose estimation model\n",
    "\n",
    "    return preprocessed_image\n",
    "\n",
    "\n",
    "def text_detection(input_image):\n",
    "    '''\n",
    "    Given some input image, preprocess the image so that\n",
    "    it can be used with the related text detection model\n",
    "    you downloaded previously. You can use cv2.resize()\n",
    "    to resize the image.\n",
    "    '''\n",
    "    preprocessed_image = np.copy(input_image)\n",
    "\n",
    "    # TODO: Preprocess the image for the text detection model\n",
    "\n",
    "    return preprocessed_image\n",
    "\n",
    "\n",
    "def car_meta(input_image):\n",
    "    '''\n",
    "    Given some input image, preprocess the image so that\n",
    "    it can be used with the related car metadata model\n",
    "    you downloaded previously. You can use cv2.resize()\n",
    "    to resize the image.\n",
    "    '''\n",
    "    preprocessed_image = np.copy(input_image)\n",
    "\n",
    "    # TODO: Preprocess the image for the car metadata model\n",
    "\n",
    "    return preprocessed_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Pre-processing Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/erNsB5nXgW4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because all of the models need the same preprocessing, except the height and width of the input of the networks. We fetch the images with `cv2.imread` and comes in the BGR format. Because our models want BGR inputs, we don't need to convert them to RGB format. However we need to reshape images because they come as `height x width x channels` order, but our model networks want channels first, along with an extra dimension at the start for batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can code a preprocessing function and call that on the other functions that used for per models' input preprocessing. \n",
    "\n",
    "In preprocessing function we can;\n",
    "1. Resize the image,\n",
    "2. Move the channels from last to first,\n",
    "3. Add an extra dimension of `1` to the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def preprocessing(input_image, height, width):\n",
    "    image = cv2.resize(input_image, (width,height)) # width, height\n",
    "    image = image.transpose((2,0,1))\n",
    "    image = image.reshape(1, 3, height, width)\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "def pose_estimation(input_image):\n",
    "    '''\n",
    "    Given some input image, preprocess the image so that\n",
    "    it can be used with the related pose estimation model\n",
    "    you downloaded previously. You can use cv2.resize()\n",
    "    to resize the image.\n",
    "    '''\n",
    "    preprocessed_image = np.copy(input_image)\n",
    "\n",
    "    # TODO: Preprocess the image for the pose estimation model\n",
    "    preprocessed_image = preprocessing(preprocessed_image, 256, 456)\n",
    "\n",
    "    return preprocessed_image\n",
    "\n",
    "\n",
    "def text_detection(input_image):\n",
    "    '''\n",
    "    Given some input image, preprocess the image so that\n",
    "    it can be used with the related text detection model\n",
    "    you downloaded previously. You can use cv2.resize()\n",
    "    to resize the image.\n",
    "    '''\n",
    "    preprocessed_image = np.copy(input_image)\n",
    "\n",
    "    # TODO: Preprocess the image for the text detection model\n",
    "    preprocessed_image = preprocessing(preprocessed_image, 768, 1280)\n",
    "\n",
    "    return preprocessed_image\n",
    "\n",
    "\n",
    "def car_meta(input_image):\n",
    "    '''\n",
    "    Given some input image, preprocess the image so that\n",
    "    it can be used with the related car metadata model\n",
    "    you downloaded previously. You can use cv2.resize()\n",
    "    to resize the image.\n",
    "    '''\n",
    "    preprocessed_image = np.copy(input_image)\n",
    "\n",
    "    # TODO: Preprocess the image for the car metadata model\n",
    "    preprocessed_image = preprocessing(preprocessed_image, 72, 72)\n",
    "\n",
    "    return preprocessed_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!!! Testing section can be added here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Network Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/pREe4P5yygM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw three type of computer vision model outputs:\n",
    "- Classes\n",
    "- Bounding boxes\n",
    "- Semantic labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification networks** typically output an array with the softmax probabilities of classes. Those probabilities become meaningful using `argmax` function. So we achieve an array of class predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bounding boxes** generally come out from multiple bounding box detection models. Each box has a class and confidence. We can ignore boxes that has low confidence. \n",
    "\n",
    "Each bounding box has for values for location:\n",
    "- Whether they can be X, Y pairs of opposite corner pairs of bounding boxes,\n",
    "- or otherwise one X, Y values of a corner and height and width of bounding boxes\n",
    "\n",
    "Further Research for [SSD and its output](https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semantic labels** give the class for each pixel. Sometimes output gives a flattened version of them, or a different size than the original image. In these situations we need to reshape or resize them to map directly back to the input.\n",
    "\n",
    "Further Research for [Semantic Segmentation](https://thegradient.pub/semantic-segmentation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "In a network like SSD, the output is a series of bounding boxes for potential object detections with a confidence threshold or how confident the model is about that particular detection.\n",
    "\n",
    "Let's assume we have an output array with bounding box predictions. \n",
    "\n",
    "The array includes below ordinally:\n",
    "- The class of the object,\n",
    "- The confidence,\n",
    "- Two corners (xmin, ymin, xmax, ymax)\n",
    "\n",
    "We can extract the bounding boxes from a given network output using the script below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for box in output:\n",
    "    if box[1] > conf_threshold:\n",
    "        xmin = int(box[2] * width)\n",
    "        ymin = int(box[3] * height)\n",
    "        xmax = int(box[4] * width)\n",
    "        ymax = int(box[5] * height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Our First Edge App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/FANZZXUqGac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have familiarity with pre-trained models, preprocessing inputs for it and handling its output. In the next exercise, we'll load a pre-trained model into the Inference Engine with its preprocessing and output handling functions in the appropriate locations. So we'll run our first edge application.\n",
    "\n",
    "We are still abstracting some steps of dealing with the Inference Engine API. We'll dive deep into it, but for now, we can try to understand the concept. The concept is implemented similarly across different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Deploy An App at the Edge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We downloaded some pre-trained models and we made preprocessing operations on the input images.\n",
    "Now we deploy our first application in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of code behind the scenes here. So we need to understand the concept in this exercise. We'll understand deeply on further parts of our tutorial.\n",
    "\n",
    "We don't use Model Optimizer in this exercise, instead we use pre-trained models. We'll load our models into the Inference Engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want a sneak preview of some of the code that interfaces with the Inference Engine, we can check out the code below as `inference.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Contains code for working with the Inference Engine.\n",
    "You'll learn how to implement this code and more in\n",
    "the related lesson on the topic.\n",
    "'''\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging as log\n",
    "from openvino.inference_engine import IENetwork, IECore\n",
    "\n",
    "class Network:\n",
    "    '''\n",
    "    Load and store information for working with the Inference Engine,\n",
    "    and any loaded models.\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.plugin = None\n",
    "        self.input_blob = None\n",
    "        self.exec_network = None\n",
    "\n",
    "\n",
    "    def load_model(self, model, device=\"CPU\", cpu_extension=None):\n",
    "        '''\n",
    "        Load the model given IR files.\n",
    "        Defaults to CPU as device for use in the workspace.\n",
    "        Synchronous requests made within.\n",
    "        '''\n",
    "        model_xml = model\n",
    "        model_bin = os.path.splitext(model_xml)[0] + \".bin\"\n",
    "\n",
    "        # Initialize the plugin\n",
    "        self.plugin = IECore()\n",
    "\n",
    "        # Add a CPU extension, if applicable\n",
    "        if cpu_extension and \"CPU\" in device:\n",
    "            self.plugin.add_extension(cpu_extension, device)\n",
    "\n",
    "        # Read the IR as a IENetwork\n",
    "        network = IENetwork(model=model_xml, weights=model_bin)\n",
    "\n",
    "        # Load the IENetwork into the plugin\n",
    "        self.exec_network = self.plugin.load_network(network, device)\n",
    "\n",
    "        # Get the input layer\n",
    "        self.input_blob = next(iter(network.inputs))\n",
    "\n",
    "        # Return the input shape (to determine preprocessing)\n",
    "        return network.inputs[self.input_blob].shape\n",
    "\n",
    "\n",
    "    def sync_inference(self, image):\n",
    "        '''\n",
    "        Makes a synchronous inference request, given an input image.\n",
    "        '''\n",
    "        self.exec_network.infer({self.input_blob: image})\n",
    "        return\n",
    "\n",
    "\n",
    "    def extract_output(self):\n",
    "        '''\n",
    "        Returns a list of the results for the output layer of the network.\n",
    "        '''\n",
    "        return self.exec_network.requests[0].outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll work on `handle_models.py` file below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def handle_pose(output, input_shape):\n",
    "    '''\n",
    "    Handles the output of the Pose Estimation model.\n",
    "    Returns ONLY the keypoint heatmaps, and not the Part Affinity Fields.\n",
    "    '''\n",
    "    # TODO 1: Extract only the second blob output (keypoint heatmaps)\n",
    "    heatmaps = output['Mconv7_stage2_L2']\n",
    "    #print(heatmaps.shape)\n",
    "    # TODO 2: Resize the heatmap back to the size of the input\n",
    "    out_heatmap = np.zeros([heatmaps.shape[1], input_shape[0], \n",
    "                            input_shape[1]])\n",
    "    # Iterate through and re-size each heatmap\n",
    "    for h in range(len(heatmaps[0])):\n",
    "        out_heatmap[h] = cv2.resize(heatmaps[0][h], \n",
    "                                   input_shape[0:2][::-1])\n",
    "\n",
    "    return out_heatmap\n",
    "\n",
    "\n",
    "def handle_text(output, input_shape):\n",
    "    '''\n",
    "    Handles the output of the Text Detection model.\n",
    "    Returns ONLY the text/no text classification of each pixel,\n",
    "        and not the linkage between pixels and their neighbors.\n",
    "    '''\n",
    "    # TODO 1: Extract only the first blob output (text/no text classification)\n",
    "    text_classes = output['model/segm_logits/add']\n",
    "    # TODO 2: Resize this output back to the size of the input\n",
    "    out_text = np.empty([text_classes.shape[1], input_shape[0], input_shape[1]])\n",
    "    for t in range(len(text_classes[0])):\n",
    "        out_text[t] = cv2.resize(text_classes[0][t], input_shape[0:2][::-1])\n",
    "\n",
    "    return out_text\n",
    "\n",
    "\n",
    "def handle_car(output, input_shape):\n",
    "    '''\n",
    "    Handles the output of the Car Metadata model.\n",
    "    Returns two integers: the argmax of each softmax output.\n",
    "    The first is for color, and the second for type.\n",
    "    '''\n",
    "    # TODO 1: Get the argmax of the \"color\" output\n",
    "    #print(output.keys())\n",
    "    color = output['color'].flatten()\n",
    "    car_type = output['type'].flatten()\n",
    "    #print(color.shape)\n",
    "    #print(car_type.shape)\n",
    "    color_class = np.argmax(color)\n",
    "    # TODO 2: Get the argmax of the \"type\" output\n",
    "    type_class = np.argmax(car_type)\n",
    "\n",
    "    return color_class, type_class\n",
    "\n",
    "\n",
    "def handle_output(model_type):\n",
    "    '''\n",
    "    Returns the related function to handle an output,\n",
    "        based on the model_type being used.\n",
    "    '''\n",
    "    if model_type == \"POSE\":\n",
    "        return handle_pose\n",
    "    elif model_type == \"TEXT\":\n",
    "        return handle_text\n",
    "    elif model_type == \"CAR_META\":\n",
    "        return handle_car\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "'''\n",
    "The below function is carried over from the previous exercise.\n",
    "You just need to call it appropriately in `app.py` to preprocess\n",
    "the input image.\n",
    "'''\n",
    "def preprocessing(input_image, height, width):\n",
    "    '''\n",
    "    Given an input image, height and width:\n",
    "    - Resize to width and height\n",
    "    - Transpose the final \"channel\" dimension to be first\n",
    "    - Reshape the image to add a \"batch\" of 1 at the start \n",
    "    '''\n",
    "    image = np.copy(input_image)\n",
    "    image = cv2.resize(image, (width, height))\n",
    "    image = image.transpose((2,0,1))\n",
    "    image = image.reshape(1, 3, height, width)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we call functions from our edge app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from handle_models import handle_output, preprocessing\n",
    "from inference import Network\n",
    "\n",
    "\n",
    "CAR_COLORS = [\"white\", \"gray\", \"yellow\", \"red\", \"green\", \"blue\", \"black\"]\n",
    "CAR_TYPES = [\"car\", \"bus\", \"truck\", \"van\"]\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    '''\n",
    "    Gets the arguments from the command line.\n",
    "    '''\n",
    "\n",
    "    parser = argparse.ArgumentParser(\"Basic Edge App with Inference Engine\")\n",
    "    # -- Create the descriptions for the commands\n",
    "\n",
    "    c_desc = \"CPU extension file location, if applicable\"\n",
    "    d_desc = \"Device, if not CPU (GPU, FPGA, MYRIAD)\"\n",
    "    i_desc = \"The location of the input image\"\n",
    "    m_desc = \"The location of the model XML file\"\n",
    "    t_desc = \"The type of model: POSE, TEXT or CAR_META\"\n",
    "\n",
    "    # -- Add required and optional groups\n",
    "    parser._action_groups.pop()\n",
    "    required = parser.add_argument_group('required arguments')\n",
    "    optional = parser.add_argument_group('optional arguments')\n",
    "\n",
    "    # -- Create the arguments\n",
    "    required.add_argument(\"-i\", help=i_desc, required=True)\n",
    "    required.add_argument(\"-m\", help=m_desc, required=True)\n",
    "    required.add_argument(\"-t\", help=t_desc, required=True)\n",
    "    optional.add_argument(\"-c\", help=c_desc, default=None)\n",
    "    optional.add_argument(\"-d\", help=d_desc, default=\"CPU\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def get_mask(processed_output):\n",
    "    '''\n",
    "    Given an input image size and processed output for a semantic mask,\n",
    "    returns a masks able to be combined with the original image.\n",
    "    '''\n",
    "    # Create an empty array for other color channels of mask\n",
    "    empty = np.zeros(processed_output.shape)\n",
    "    # Stack to make a Green mask where text detected\n",
    "    mask = np.dstack((empty, processed_output, empty))\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_output_image(model_type, image, output):\n",
    "    '''\n",
    "    Using the model type, input image, and processed output,\n",
    "    creates an output image showing the result of inference.\n",
    "    '''\n",
    "    if model_type == \"POSE\":\n",
    "        # Remove final part of output not used for heatmaps\n",
    "        output = output[:-1]\n",
    "        # Get only pose detections above 0.5 confidence, set to 255\n",
    "        for c in range(len(output)):\n",
    "            output[c] = np.where(output[c]>0.5, 255, 0)\n",
    "        # Sum along the \"class\" axis\n",
    "        output = np.sum(output, axis=0)\n",
    "        # Get semantic mask\n",
    "        pose_mask = get_mask(output)\n",
    "        # Combine with original image\n",
    "        image = image + pose_mask\n",
    "        return image\n",
    "    elif model_type == \"TEXT\":\n",
    "        # Get only text detections above 0.5 confidence, set to 255\n",
    "        output = np.where(output[1]>0.5, 255, 0)\n",
    "        # Get semantic mask\n",
    "        text_mask = get_mask(output)\n",
    "        # Add the mask to the image\n",
    "        image = image + text_mask\n",
    "        return image\n",
    "    elif model_type == \"CAR_META\":\n",
    "        # Get the color and car type from their lists\n",
    "        color = CAR_COLORS[output[0]]\n",
    "        car_type = CAR_TYPES[output[1]]\n",
    "        # Scale the output text by the image shape\n",
    "        scaler = max(int(image.shape[0] / 1000), 1)\n",
    "        # Write the text of color and type onto the image\n",
    "        image = cv2.putText(image, \n",
    "            \"Color: {}, Type: {}\".format(color, car_type), \n",
    "            (50 * scaler, 100 * scaler), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "            2 * scaler, (255, 255, 255), 3 * scaler)\n",
    "        return image\n",
    "    else:\n",
    "        print(\"Unknown model type, unable to create output image.\")\n",
    "        return image\n",
    "\n",
    "\n",
    "def perform_inference(args):\n",
    "    '''\n",
    "    Performs inference on an input image, given a model.\n",
    "    '''\n",
    "    # Create a Network for using the Inference Engine\n",
    "    inference_network = Network()\n",
    "    # Load the model in the network, and obtain its input shape\n",
    "    n, c, h, w = inference_network.load_model(args.m, args.d, args.c)\n",
    "\n",
    "    # Read the input image\n",
    "    image = cv2.imread(args.i)\n",
    "\n",
    "    ### TODO: Preprocess the input image\n",
    "    preprocessed_image = preprocessing(image, h, w)\n",
    "\n",
    "    # Perform synchronous inference on the image\n",
    "    inference_network.sync_inference(preprocessed_image)\n",
    "\n",
    "    # Obtain the output of the inference request\n",
    "    output = inference_network.extract_output()\n",
    "\n",
    "    ### TODO: Handle the output of the network, based on args.t\n",
    "    ### Note: This will require using `handle_output` to get the correct\n",
    "    ###       function, and then feeding the output to that function.\n",
    "    preprocess_func = handle_output(args.t)\n",
    "    processed_output = preprocess_func(output, image.shape)\n",
    "\n",
    "    # Create an output image based on network\n",
    "    try:\n",
    "        output_image = create_output_image(args.t, image, processed_output)\n",
    "        print(\"Success\")\n",
    "    except:\n",
    "        output_image = image\n",
    "        print(\"Error!\")\n",
    "\n",
    "    # Save down the resulting image\n",
    "    cv2.imwrite(\"outputs/{}-output.png\".format(args.t), output_image)\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = get_args()\n",
    "    perform_inference(args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Deploy an App at the Edge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/X9yI7U2Rn00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Youtube Video](https://youtu.be/o-fWs0BwbyM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Glossary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applications that make almost all processing at the Edge is expressed as Edge Applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenVINO™ Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Precision"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
